
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo=TRUE)
library(knitr)
knitr::opts_chunk$set(echo=TRUE)
knitr::knit_hooks$set(mysize=function(before, options, envir){
  if (before)
    return(options$size)
})
library(Sleuth3)
library(tidyverse)
library(ggplot2)
library(dplyr)
```

Simple linear regression examines relationships between two variables. The *y*-variable is the outcome, the criterion, the dependent variable, or the response variable. The *x*-variable is the predictor, the regressor, the independent variable, or the explanatory variable. We will find the regression equation that gives us the best prediction of the response from the explanatory variable.

Researchers desire to know whether walking velocity (meters per second) is predictive of the velocity required to climb the stairs (stair per-second). The response variable is the velocity required to climb the stairs and walking velocity is the explanatory variable.

$Y=\beta_{0}+\beta_{1}X+\epsilon$, where $\epsilon\sim N(0, \sigma^2)$. The assumptions that are implied by the linear regression model include normality of residuals, equal variance across a range of *x*-values, independence, and linearity.

The parameters $\beta_{0}$ and $\beta_{1}$ are called the regression coefficients. $\beta_{0}$ is the intercept, and is equal to the expected (average) value of *y* when $x=0$. $\beta_{1}$ is the slope, the expected difference in *y* per unit difference in *x*. So, for two observations with $x_{1}=x_{2}+1$, on average, $y_{1}=y_{2}+\beta_{1}$.

Consider the linear relationship with $E[\text{stairs}/\text{second}]=0.2+0.4(\text{meters}/\text{second})$.

```{r, echo=FALSE}
par(mfrow=c(2,2))
x=1:3
y=1:3
plot(x, y, xlim=c(0,3), ylim=c(0,3))

x=c(1,1,2)
y=c(1,2,2)
plot(x, y, xlim=c(0,3), ylim=c(0,3))

x=c(1,1,2,2)
y=c(1,2,1,2)
plot(x, y, xlim=c(0,3), ylim=c(0,3))

x=c(1,2,3,3,4,5)
y=c(2,3,3,1,2,3)
plot(x, y, xlim=c(0,6), ylim=c(0,4))
abline(lm(y~x), col="red")
```

To decide which line fits the best, we require criteria. Here, we desire to minimize the distance from the line in some way. With the minimum total distance, since some points are above and some are below, positive and negative distances will cancel out. Therefore, we utilize squared distance, which is mathematically more efficient than absolute distance.

So, the best-fitting line is the one that minimizes the sum of squared residuals (the vertical distances from the point to the line).

$\frac{\sum^{n}_{i=1}\left(Y_{i}-\left(\beta_{1}X_{i}+\beta_{0}\right)\right)^2}{n}=\frac{\sum^{n}_{i=1}\left(Y_{i}-\hat{Y}_{i}\right)^2}{n}=\frac{\sum^{n}_{i=1}d^{2}_{i}}{n}$

Linear regression gives us a method for predicting the response for any possible value of the explanatory variable. If that value is outside the range of observed, it is extrapolation. If that value is within the range of observed values, it is interpolation.

```{r, message=FALSE, warning=FALSE}
mile <- read_csv('runner.csv')
```

Linear regression gives us a method for predicting the response for any possible value of the explanatory variable. If that value is outside the range of observed values, it is extrapolation. If that value is within the range of observed values, it is interpolation.

```{r}
mile %>%
  separate(Date, into=c("month", "day", "year"), sep="/", convert=TRUE) %>%
  mutate(date.numeric= year +(month-1)/12 +(day/31/12)) %>%
  ggplot(aes(x=date.numeric, y=time)) + geom_step() + geom_smooth(method="lm")
```

***Extrapolation***

```{r}
mile %>%
  add_row(min=3, sec=43.13, time=3+43.13/6, Date='11/16/2020') %>%
  separate(Date, into=c("month", "day", "year"), sep="/", convert=TRUE) %>%
  mutate(date.numeric= year + (month-1)/12 + (day/31/12)) %>%
  ggplot(aes(x=date.numeric, y=time)) + geom_step()
```

Linear regression gives us a method for predicting the response for any possible value of the explanatory variable. If that value is outside the range of observed, it is extrapolation. If that value is within the range of observed values, it is interpolation.

```{r loading}
hubble <- case0701
```

Hubble measured information on 24 nebulae, including their distance from Earth and the velocity at which they were moving away from Earth. Questions that a linear regression could address include the estimated age of the universe (slope) or the adequacy of the Big Bang Theory (intercept).

```{r scatterEx, fig.height=4}
ggplot(data=hubble, aes(x=Velocity, y=Distance)) + geom_point() +
  labs(x="Velocity (Kilometers Per Second)", y="Distance (Megaparsecs)")
```

Hubble measured information on 24 nebulae, including their distance from Earth and the velocity at which they were moving away from Earth. Questions that a linear regression could address include the estimated age of the universe (slope) or the adequacy of the Big Bang Theory (intercept).

```{r mysize=TRUE, size='\\footnotesize'}
fit <- lm(Distance~Velocity, data=hubble)
summary(fit)
# Intercept
summary(fit)$coef
tstar <- qt(0.975, 22)
summary(fit)$coef[2,1]-(tstar*summary(fit)$coef[2,2])
summary(fit)$coef[2,1]+(tstar*summary(fit)$coef[2,2])
# Slope
summary(fit)$coef
tstart <- qt(0.975, 22)
summary(fit)$coef[1,1]-(tstar*summary(fit)$coef[1,2])
summary(fit)$coef[1,1]+(tstar*summary(fit)$coef[1,2])
```

Hubble measured information on 24 nebulae, including their distance from Earth and the velocity at which they were moving away from Earth. Questions that a linear regression could address include the estimated age of the universe (slope) or the adequacy of the Big Bang Theory (intercept).

```{r}
confint(fit)
```

We estimate the age of the universe to be 0.0014 $\frac{\text{mpsc}*\text{sec}}{\text{km}}$, which is approximately 1.37 billion years, with the 95 percent confidence interval calculated as $(0.88, 1.80)$ billion years.

```{r, mysize=TRUE, size='\\footnotesize'}
0.0014*3.086e19*3.17098e-8/1e9
```

We have sufficient evidence that the intercept is not zero, with a 95 percent confidence interval of $(0.15, 0.65)$. So, the basic theory, which specified that distance is equal to the summation of the product of the age of the universe and velocity and zero, is not supported.

```{r, mysize=TRUE, size='\\footnotesize', figh.height=5}
plot(hubble$Velocity, hubble$Distance, xlab="Velocity (km/sec)", 
     ylab="Distance (megaparsecs)")
abline(fit$coef, col="red", lwd=2)
abline(c(0, lm(Distance~Velocity-1, data=hubble)$coef), col="blue", lwd=2)
```

Regression gives us a model for estimating the response at any possible value of the explanatory variable. Suppose we desire to know the estimated distance for a nebula moving way from Earth at 500 kilometers per second. 

```{r}
0.39917 + 0.001372*500
sum(c(1,500)*fit$coef)
```

***Estimation***

```{r, mysize=TRUE, size='\\footnotesize'}
predict(fit, newdata=data.frame(Velocity=500), interval="conf", se.fit=TRUE)
```

***Prediction***

```{r, mysize=TRUE, size='\\footnotesize'}
predict(fit, newdata=data.frame(Velocity=500), interval="prediction", se.fit=TRUE)
```

When you estimate the mean response for a specific number, $X_{0}$, such that $\hat\mu\{Y|X_{0}\}$, the standard error of $\hat\mu\{Y|X_{0}\}$ is $\hat\mu\{Y|X_{0}\}=\hat\beta_{0}+\hat\beta_{1}X_{0}=\hat\sigma\sqrt{\frac{1}{n}+\frac{(X_{0}-\overline{X})^2}{(n-1)s^{2}_{X}}}$, with $df=n-2$.

```{r}
# Plot of Confidence Intervals Versus Bands
hubble %>% ggplot(aes(x=Velocity, y=Distance)) + geom_point() + 
  geom_smooth(method="lm", se=TRUE) + labs(title="Distance Versus Velocity", 
                                           y="Distance", x="Velocity")
```

Prediction is utilized when we desire to predict the response *y* for a single individual (not the mean). A prediction interval indicates likely values for a future value of a response variable at some value of the explanatory variable. It is called such because we desire to guess the value of a random variable, not a parameter. The variability of an individual response is greater than the variability of an average response.

The standard error of $\text{PRED}\{Y|X_{0}\}=\sqrt{\hat\sigma^2+SE(\hat\mu\{Y|X_{0}\})^2}=\hat\sigma\sqrt{\frac{1}{n}+\frac{(X_{0}-\overline{X})^2}{(n-1)s^{2}_{X}}}$

```{r}
# Plot of Confidence And Prediction Intervals
temp_var <- predict(fit, interval="prediction")
new_ds <- cbind(hubble, temp_var)

new_ds %>% ggplot(aes(x=Velocity, y=Distance)) + geom_point() + 
  geom_line(aes(y=lwr), color="red", linetype="dashed") + 
  geom_line(aes(y=upr), color="red", linetype="dashed") +
  geom_smooth(method="lm", se=FALSE) + labs(title="Distance Versus Velocity",
                                            y="Distance", x="Velocity")
```

The Pearson Product Moment Correlation Coefficient measures the extent to which a straight line fits the data. It also measures how far the other points are from a line and how tightly high values on one variable are tied to high values on the other variable.

To calculate correlation, we must compute the *z*-score for each case on each variable ($z_{x}=\frac{x-\bar{x}}{sd_{x}}$). We then average the product of the *z*-scores ($r=\frac{\sum_{i=1}^n z_{x}\cdot z_{y}}{n}$). Note that a data point that is high or low on both values will contribute a large positive value; one that is high on one value  and low on the other value. The other value will contribute a large negative value.

For the Pearson Product Moment Correlation Coefficient, the value is always between $-1$ and $1$. A perfect line with a positive slope is represented by $r=1$. A perfect line with a negative slope is represented by $r=-1$. If there is no linear relationship, $r=0$. If $r<0.1$, there is no relationship. If $0.1\leq r<0.3$, the relationship is weak. If $0.3\leq r<0.5$, the relationship is moderate. If $r\geq0.5$, the relationship is strong. Correlation is ordinal. Correlation does not imply causation.

Given the relationship between the correlation and slope in simple linear regression, it should not be surprising that *r* can also be utilized as a measure of the adequacy of the model. We utilize $r^2$ to assess model fit. Known as the coefficient of determination, $r^2$ is interpreted as the proportion of the variability in the response that can be accounted for by its linear relationship with the explanatory variable(s).

```{r, echo=FALSE, mysize=TRUE, size='\\footnotesize'}
nebulae <- case0701
summary(lm(Distance~Velocity, data=nebulae))
```

Correlation is ordinal. Correlation does not imply causation.

```{r warnining=FALSE, message=FALSE}
hubble <- case0701
head(hubble)
fitx1 <- lm(Distance~Velocity, data=hubble)
summary(fitx1)

insects <- read_csv('insects.csv')
head(insects)
lm.fit <- lm(Insects~Color, data=insects)
summary(lm.fit)

aov.fit <- aov(Insects~Color, data=insects)
summary(aov.fit)
summary(lm.fit)
anova(lm.fit)

lm.fit0 <- lm(Insects~Color-1, data=insects)
summary(lm.fit0)
```

Correlation is ordinal. Correlation does not imply causation.

```{r, mysize=TRUE, size='\\footnotesize'}
predict(lm.fit, newdata=data.frame(Color='blue'), interval="conf", se.fit=TRUE)
predict(lm.fit, newdata=data.frame(Color='blue'), interval="prediction", se.fit=TRUE)
```

Correlation is ordinal. Correlation does not imply causation.

```{r warning=FALSE, message=FALSE}
par(mfrow=c(1,2))
plot(lm.fit, which=c(1,2))
insects %>%
  ggplot(aes(x=Color, y=Insects)) + geom_point()
levels(factor(insects$Color))
insects$Color <- factor(factor(insects$Color), levels=c("blue, white", "green", "lemon"))
insects %>%
  ggplot(aes(x=Color, y=Insects)) + geom_point()
as.numeric(insects$Color)
insects %>%
  ggplot(aes(x=as.numeric(Color), y=Insects)) + geom_point() + 
  geom_smooth(method="lm", se=FALSE, col="blue")
```

